{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f9c692a",
   "metadata": {},
   "source": [
    "# Multi-Agent Collaboration System\n",
    "\n",
    "This notebook demonstrates how to build a multi-agent collaborative system using LlamaIndex, where different specialized agents work together to complete complex tasks.\n",
    "\n",
    "## Prerequisites\n",
    "- An OpenAI API key\n",
    "- Notion integration token (optional, for Notion research)\n",
    "- Understanding of LlamaIndex workflows and agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b97c6f",
   "metadata": {},
   "source": [
    "# Warning\n",
    "This notebook first run can take up to 5 minutes to run due to the Notion API. After the first run, it will be much faster as the data will be cached. If you want to speed up the process, you can use a local file or a different data source instead of Notion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe34e96",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages. Run the cell below to install all dependencies needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ee580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install llama-index llama-index-llms-openai llama-index-readers-notion python-dotenv tavily-python nest-asyncio openai\n",
    "\n",
    "# Verify installations\n",
    "import importlib\n",
    "\n",
    "def check_package(package_name):\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "packages = {\n",
    "    \"llama_index\": \"llama-index core\",\n",
    "    \"llama_index.llms.openai\": \"OpenAI integration\",\n",
    "    \"llama_index.readers.notion\": \"Notion reader\",\n",
    "    \"tavily\": \"Tavily search\",\n",
    "    \"nest_asyncio\": \"Nested async loop support\",\n",
    "    \"openai\": \"OpenAI API\"\n",
    "}\n",
    "\n",
    "all_installed = True\n",
    "for package, display_name in packages.items():\n",
    "    installed = check_package(package)\n",
    "    print(f\"{display_name}: {'✅ Installed' if installed else '❌ Not installed'}\")\n",
    "    all_installed = all_installed and installed\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\n✅ All required packages are installed!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Some packages are missing. Run the installation command again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c444f1c",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Load environment variables from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "NOTION_INTEGRATION_TOKEN = os.getenv(\"NOTION_INTEGRATION_TOKEN\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Set environment variables for compatibility with libraries that expect them\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY or \"\"\n",
    "os.environ[\"NOTION_INTEGRATION_TOKEN\"] = NOTION_INTEGRATION_TOKEN or \"\"\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY or \"\"\n",
    "\n",
    "# Verify API keys are set\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"⚠️ Warning: OPENAI_API_KEY is not set in .env file\")\n",
    "else:\n",
    "    print(\"✅ OpenAI API key is set\")\n",
    "\n",
    "if not NOTION_INTEGRATION_TOKEN:\n",
    "    print(\"⚠️ Warning: NOTION_INTEGRATION_TOKEN is not set in .env file (optional for Notion research)\")\n",
    "else:\n",
    "    print(\"✅ Notion integration token is set\")\n",
    "\n",
    "if not TAVILY_API_KEY:\n",
    "    print(\"⚠️ Warning: TAVILY_API_KEY is not set in .env file (optional for web search)\")\n",
    "else:\n",
    "    print(\"✅ Tavily API key is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc7d2f",
   "metadata": {},
   "source": [
    "## Setup Language Model\n",
    "\n",
    "We'll configure OpenAI's language model for our agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d64536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Configure OpenAI model with API key from environment\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "print(\"✅ Language model configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8bd24",
   "metadata": {},
   "source": [
    "## Define Research Tools\n",
    "\n",
    "Let's define tools our agents can use to gather information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102efbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import AsyncTavilyClient \n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "from llama_index.readers.notion import NotionPageReader\n",
    "\n",
    "async def search_web(query: str) -> str:\n",
    "    \"\"\"Useful for using the web to answer questions.\"\"\"\n",
    "    client = AsyncTavilyClient(api_key=TAVILY_API_KEY)\n",
    "    return str(await client.search(query))\n",
    "\n",
    "\n",
    "async def notion_search(query: str) -> str:\n",
    "    \"\"\"Search in Notion with caching for both documents and vector index.\"\"\"\n",
    "    # Specify your Notion page IDs here\n",
    "    page_ids = [\"9917363395904835a604ca7a6a358579\"]  # Replace with your actual page ID\n",
    "    cache_folder = \"cache\"\n",
    "    os.makedirs(cache_folder, exist_ok=True)\n",
    "    \n",
    "    # Cache files\n",
    "    doc_cache_file = os.path.join(cache_folder, \"notion_data.pkl\")\n",
    "    index_cache_file = os.path.join(cache_folder, \"vector_index.pkl\")\n",
    "    \n",
    "    one_week_seconds = 7 * 24 * 60 * 60\n",
    "    update_cache = True\n",
    "\n",
    "    # Check documents cache\n",
    "    if os.path.exists(doc_cache_file):\n",
    "        file_age = time.time() - os.path.getmtime(doc_cache_file)\n",
    "        if file_age < one_week_seconds:\n",
    "            update_cache = False\n",
    "\n",
    "    if update_cache:\n",
    "        print(\"Fetching new data from Notion...\", flush=True)\n",
    "        documents = NotionPageReader(integration_token=NOTION_INTEGRATION_TOKEN).load_data(\n",
    "            page_ids=page_ids\n",
    "        )\n",
    "        with open(doc_cache_file, \"wb\") as f:\n",
    "            pickle.dump(documents, f)\n",
    "        # If documents were updated, we also recalc the index.\n",
    "        update_cache_index = True\n",
    "    else:\n",
    "        print(\"Loading data from cache...\", flush=True)\n",
    "        with open(doc_cache_file, \"rb\") as f:\n",
    "            documents = pickle.load(f)\n",
    "        update_cache_index = not os.path.exists(index_cache_file)  # Recalc index if not cached\n",
    "\n",
    "    # Setup text splitter (splitting the text into chunks)\n",
    "    from llama_index.core.node_parser import SentenceSplitter\n",
    "    from llama_index.core import Settings\n",
    "    text_splitter = SentenceSplitter(chunk_size=100, chunk_overlap=10)\n",
    "    Settings.text_splitter = text_splitter\n",
    "\n",
    "    # Load or build vector index\n",
    "    if update_cache_index:\n",
    "        from llama_index.core import VectorStoreIndex\n",
    "        index = VectorStoreIndex.from_documents(documents, transformations=[text_splitter])\n",
    "        with open(index_cache_file, \"wb\") as f:\n",
    "            pickle.dump(index, f)\n",
    "    else:\n",
    "        print(\"Loading vector index from cache...\", flush=True)\n",
    "        with open(index_cache_file, \"rb\") as f:\n",
    "            index = pickle.load(f)\n",
    "    \n",
    "    # Limit results to the top 3 matches\n",
    "    query_engine = index.as_query_engine(llm=llm, similarity_top_k=3)\n",
    "    response = query_engine.query(query)\n",
    "     \n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad1de0",
   "metadata": {},
   "source": [
    "## Create Multi-Agent System\n",
    "\n",
    "Now we'll define multiple specialized agents that will collaborate on tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc28740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgent as GenericFunctionCallingAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# Convert our search functions into tools\n",
    "search_web_tool = FunctionTool.from_defaults(fn=search_web)\n",
    "search_notion_tool = FunctionTool.from_defaults(fn=notion_search)\n",
    "\n",
    "# Research agent - responsible for information gathering\n",
    "research_agent = GenericFunctionCallingAgent.from_tools(\n",
    "    tools=[search_web_tool, search_notion_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    system_prompt=\"You are an agent that researches information from various sources including Notion and the web. \"\\\n",
    "               \"Your goal is to gather comprehensive and accurate information on a given topic.\"\n",
    ")\n",
    "\n",
    "# Writing agent - responsible for composing responses\n",
    "write_agent = GenericFunctionCallingAgent.from_tools(\n",
    "    tools=[],\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    system_prompt=\"You are an agent that writes clear, detailed, and well-structured answers \"\\\n",
    "               \"based on research provided to you. Format your responses appropriately.\"\n",
    ")\n",
    "\n",
    "# Review agent - responsible for quality assurance\n",
    "review_agent = GenericFunctionCallingAgent.from_tools(\n",
    "    tools=[],\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    "    allow_parallel_tool_calls=False,\n",
    "    system_prompt=\"You are an agent that critically reviews answers for accuracy, clarity, and completeness. \"\\\n",
    "               \"You should identify any issues, inaccuracies, or areas for improvement.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aefec5",
   "metadata": {},
   "source": [
    "## Define Workflow Events and Steps\n",
    "\n",
    "We'll now set up the workflow that our agents will follow to collaborate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf3d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Event, Context\n",
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    ")\n",
    "\n",
    "class ResearchEvent(Event):\n",
    "    prompt: str\n",
    "\n",
    "class ReviewEvent(Event):\n",
    "    answer: str\n",
    "\n",
    "class ReviewResults(Event):\n",
    "    review: str\n",
    "\n",
    "class RewriteEvent(Event):\n",
    "    review: str\n",
    "\n",
    "class WriteEvent(Event):\n",
    "    pass\n",
    "\n",
    "class MultiAgentFlowWithReflection(Workflow):\n",
    "\n",
    "    @step\n",
    "    async def setup(self, ctx: Context, ev: StartEvent) -> ResearchEvent:\n",
    "        self.research_agent = ev.research_agent\n",
    "        self.write_agent = ev.write_agent\n",
    "        self.review_agent = ev.review_agent\n",
    "        # Initialize rewriting counter and store the original prompt\n",
    "        await ctx.set(\"rewrite_count\", 0)\n",
    "        await ctx.set(\"prompt\", ev.prompt)\n",
    "        return ResearchEvent(prompt=ev.prompt)\n",
    "\n",
    "    @step\n",
    "    async def research(self, ctx: Context, ev: ResearchEvent) -> WriteEvent:\n",
    "        await ctx.set(\"prompt\", ev.prompt)\n",
    "        result = self.research_agent.chat(\n",
    "            f\"Gather some information that another agent will use to write an answer about this topic: <topic>{ev.prompt}</topic>. \"\n",
    "            \"Just include the facts without making it into a full answer.\"\n",
    "        )\n",
    "        print(\"Research result:\", result)\n",
    "        await ctx.set(\"research\", str(result))\n",
    "        return WriteEvent()\n",
    "\n",
    "    @step\n",
    "    async def write(self, ctx: Context, ev: WriteEvent | RewriteEvent) -> ReviewEvent:\n",
    "        original_prompt = await ctx.get(\"prompt\")\n",
    "        research_info = await ctx.get(\"research\")\n",
    "        prompt = (\n",
    "            f\"Write a detailed, clear, and direct answer addressing the question: \"\n",
    "            f\"<question>{original_prompt}</question>. Use the following research as supporting information: \"\n",
    "            f\"<research>{research_info}</research>\"\n",
    "        )\n",
    "        if isinstance(ev, RewriteEvent):\n",
    "            print(\"Doing a rewrite!\")\n",
    "            prompt += (\n",
    "                f\" Note: This answer has been reviewed and the reviewer provided the following feedback that \"\n",
    "                f\"should be taken into account: <review>{ev.review}</review>\"\n",
    "            )\n",
    "        result = self.write_agent.chat(prompt)\n",
    "        return ReviewEvent(answer=str(result))\n",
    "\n",
    "    @step\n",
    "    async def review(self, ctx: Context, ev: ReviewEvent) -> StopEvent | RewriteEvent:\n",
    "        result = self.review_agent.chat(f\"Review this answer: {ev.answer}\")\n",
    "        original_prompt = await ctx.get(\"prompt\")\n",
    "        research_info = await ctx.get(\"research\")\n",
    "        rewrite_count = await ctx.get(\"rewrite_count\")\n",
    "        rewrite_count += 1\n",
    "        await ctx.set(\"rewrite_count\", rewrite_count)\n",
    "        try_again = llm.complete(\n",
    "            f\"This is a review of an answer written by an agent. If you think this review is bad enough \"\n",
    "            f\"that the agent should try again, respond with just the word RETRY. If the review is good, reply \"\n",
    "            f\"with just the word CONTINUE. Here's the review: <review>{str(result)}</review>\"\n",
    "        )\n",
    "        if try_again.text == \"RETRY\":\n",
    "            if rewrite_count > 3:\n",
    "                print(\"Maximum rewrite attempts reached. Finishing the flow.\")\n",
    "                return StopEvent(result=ev.answer)\n",
    "            print(\"Reviewer said try again\")\n",
    "            return RewriteEvent(\n",
    "                review=(\n",
    "                    f\"{str(result)}\\n\"\n",
    "                    f\"Original prompt: {original_prompt}\\n\"\n",
    "                    f\"Research info: {research_info}\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(\"Reviewer thought it was good!\")\n",
    "            return StopEvent(result=ev.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5722fae",
   "metadata": {},
   "source": [
    "## Run the Multi-Agent Workflow\n",
    "\n",
    "Let's run our collaborative multi-agent system on a sample question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c046f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "workflow = MultiAgentFlowWithReflection(timeout=1200, verbose=True)\n",
    "handler = workflow.run(\n",
    "    prompt=\"The roles of trees on Earth?\",  # You can change this prompt to any question you want to ask\n",
    "    research_agent=research_agent,\n",
    "    write_agent=write_agent,\n",
    "    review_agent=review_agent\n",
    ")\n",
    "\n",
    "final_result = await handler\n",
    "print(\"\\n==== Final Answer ====\")\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934836d",
   "metadata": {},
   "source": [
    "## Customizing the Multi-Agent System\n",
    "\n",
    "You can modify the system by changing the prompts, adding new agents, or adjusting the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec88dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run the workflow with a different prompt\n",
    "another_workflow = MultiAgentFlowWithReflection(timeout=120, verbose=True)\n",
    "another_handler = another_workflow.run(\n",
    "    prompt=\"What are the key principles of sustainable architecture?\",  # Different question\n",
    "    research_agent=research_agent,\n",
    "    write_agent=write_agent,\n",
    "    review_agent=review_agent\n",
    ")\n",
    "\n",
    "another_result = await another_handler\n",
    "print(\"\\n==== Final Answer ====\")\n",
    "print(another_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13d543d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to build a multi-agent collaborative system using LlamaIndex that:\n",
    "\n",
    "1. Uses specialized agents for different tasks (research, writing, and review)\n",
    "2. Implements a workflow for agents to collaborate sequentially\n",
    "3. Includes a feedback loop where content is reviewed and potentially rewritten\n",
    "4. Leverages external knowledge sources like Notion and web search\n",
    "\n",
    "This approach can be extended to more complex workflows and specialized agents for various domains and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24246dc",
   "metadata": {},
   "source": [
    "## Troubleshooting Tips\n",
    "\n",
    "If you encounter issues:\n",
    "\n",
    "1. **API Keys**: Verify that your API keys are correct and have the necessary permissions\n",
    "2. **Network Issues**: Make sure you have internet connectivity for web searches\n",
    "3. **Notion Integration**: For Notion integration, ensure your token has access to the specified pages\n",
    "4. **Async Runtime**: If you encounter async-related errors, ensure nest_asyncio is properly applied\n",
    "5. **Dependencies**: Make sure all required packages are installed:\n",
    "   ```\n",
    "   pip install llama-index llama-index-llms-openai llama-index-readers-notion python-dotenv tavily-python nest-asyncio openai\n",
    "   ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
