{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48cef4fb",
   "metadata": {},
   "source": [
    "# Simple Agent System\n",
    "\n",
    "This notebook demonstrates how to create a simple agent system using the LlamaIndex framework. This example uses a function-calling agent that can use tools to search and retrieve information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808d9e68",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages and verify the installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install llama-index llama-index-llms-openai nest-asyncio python-dotenv\n",
    "\n",
    "# Verify installations\n",
    "import importlib\n",
    "\n",
    "def check_package(package_name):\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "packages = {\n",
    "    \"llama_index\": \"LlamaIndex Core\",\n",
    "    \"llama_index.llms.openai\": \"OpenAI integration\",\n",
    "    \"nest_asyncio\": \"Nest AsyncIO\",\n",
    "    \"dotenv\": \"Python-dotenv\"\n",
    "}\n",
    "\n",
    "all_installed = True\n",
    "for package, display_name in packages.items():\n",
    "    installed = check_package(package)\n",
    "    print(f\"{display_name}: {'✅ Installed' if installed else '❌ Not installed'}\")\n",
    "    all_installed = all_installed and installed\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\n✅ All required packages are installed!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Some packages are missing. Run the installation command again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ad83a",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Load environment variables from the `.env` file. <br>\n",
    "N.b. it will look through the entire project for a valid `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f34586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# Get model name from environment or use default\n",
    "MODEL_NAME = os.getenv(\"SIMPLE_AGENT_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "# Set environment variables for compatibility with libraries that expect them\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY or \"\"\n",
    "\n",
    "# Verify API key is set\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"No OpenAI API key found. Please set OPENAI_API_KEY in your .env file.\")\n",
    "else:\n",
    "    print(\"✅ API key loaded successfully\")\n",
    "    print(f\"✅ Using model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d36699",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Let's import the necessary libraries for our agent system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a771d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4b1206",
   "metadata": {},
   "source": [
    "## Configure LLM\n",
    "\n",
    "Set up the OpenAI language model to be used by our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136befb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Initialize the language model with API key from environment\n",
    "llm = OpenAI(model=MODEL_NAME, api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Verify LLM setup with a simple test\n",
    "response = llm.complete(\"Hello, I am a language model. \")\n",
    "print(\"LLM Test Response:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b548f7",
   "metadata": {},
   "source": [
    "## Define Tools\n",
    "\n",
    "Let's define some tools that our agent can use to perform tasks. We'll create two simple tools: a calculator and a web search simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1332a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# A simple calculator tool\n",
    "async def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        # Safely evaluate the expression\n",
    "        result = eval(expression)\n",
    "        return f\"The result of {expression} is {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error evaluating {expression}: {str(e)}\"\n",
    "\n",
    "# A simulated web search tool\n",
    "async def web_search(query: str) -> str:\n",
    "    \"\"\"Simulate searching the web for information.\"\"\"\n",
    "    # This is just a simulation - in a real application, you would use an actual search API\n",
    "    search_results = {\n",
    "        \"python programming\": \"Python is a high-level, interpreted programming language known for its readability and versatility.\",\n",
    "        \"machine learning\": \"Machine learning is a field of AI that enables computers to learn from data without being explicitly programmed.\",\n",
    "        \"climate change\": \"Climate change refers to long-term shifts in temperatures and weather patterns, mainly caused by human activities.\"\n",
    "    }\n",
    "    \n",
    "    # Find the most relevant key in our mock database\n",
    "    for key in search_results:\n",
    "        if key.lower() in query.lower():\n",
    "            return search_results[key]\n",
    "    \n",
    "    return \"No relevant information found for your query.\"\n",
    "\n",
    "# Convert functions to LlamaIndex tools\n",
    "calculator_tool = FunctionTool.from_defaults(fn=calculator)\n",
    "search_tool = FunctionTool.from_defaults(fn=web_search)\n",
    "\n",
    "print(\"✅ Tools defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f56b5d",
   "metadata": {},
   "source": [
    "## Create the Agent\n",
    "\n",
    "Now let's create a function-calling agent that can use our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea52a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "\n",
    "# Create the agent with our tools\n",
    "agent = FunctionCallingAgent.from_tools(\n",
    "    tools=[calculator_tool, search_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,  # Set to True to see the agent's thought process\n",
    "    system_prompt=\"You are a helpful AI assistant that can perform calculations and search for information.\"\n",
    ")\n",
    "\n",
    "print(\"✅ Agent created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b56b3d7",
   "metadata": {},
   "source": [
    "## Test the Agent\n",
    "\n",
    "Let's try our agent with different types of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a38458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply nest_asyncio to enable asynchronous code in Jupyter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Test the agent with a calculation query\n",
    "print(\"Testing agent with calculation query...\")\n",
    "response = agent.chat(\"What is the result of 15 * 28 + 42?\")\n",
    "print(\"\\nAgent Response:\")\n",
    "display(Markdown(f\"**Answer:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a45c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with an information query\n",
    "print(\"Testing agent with information query...\")\n",
    "response = agent.chat(\"Tell me about Python programming.\")\n",
    "print(\"\\nAgent Response:\")\n",
    "display(Markdown(f\"**Answer:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69939d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a more complex query\n",
    "print(\"Testing agent with complex query...\")\n",
    "response = agent.chat(\"If I have 5 apples and each costs $1.25, how much will I pay in total? Also, can you tell me about machine learning?\")\n",
    "print(\"\\nAgent Response:\")\n",
    "display(Markdown(f\"**Answer:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5586fe",
   "metadata": {},
   "source": [
    "## Adding Memory to the Agent\n",
    "\n",
    "Let's enhance our agent by adding memory so it can remember previous interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b11e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "# Create a memory buffer\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "# Create a new agent with memory\n",
    "agent_with_memory = FunctionCallingAgent.from_tools(\n",
    "    tools=[calculator_tool, search_tool],\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    system_prompt=\"You are a helpful AI assistant that can perform calculations and search for information. You maintain context from previous interactions.\"\n",
    ")\n",
    "\n",
    "print(\"✅ Agent with memory created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b006a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test memory with a series of interactions\n",
    "print(\"Testing agent memory with first interaction...\")\n",
    "response = agent_with_memory.chat(\"My name is Alice and I'm learning about climate change.\")\n",
    "print(\"\\nAgent Response:\")\n",
    "display(Markdown(f\"**Answer:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c8eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the agent remembers the name\n",
    "print(\"Testing agent memory with follow-up question...\")\n",
    "response = agent_with_memory.chat(\"What's my name and what am I learning about?\")\n",
    "print(\"\\nAgent Response:\")\n",
    "display(Markdown(f\"**Answer:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164306c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "1. Set up a function-calling agent with LlamaIndex\n",
    "2. Define custom tools for the agent to use\n",
    "3. Test the agent with different types of queries\n",
    "4. Enhance the agent with memory capabilities\n",
    "\n",
    "This simple agent system can be expanded upon to include more sophisticated tools, such as API access, database queries, or integration with other systems.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "You can extend this example by:\n",
    "1. Adding more complex tools\n",
    "2. Integrating with real web search APIs\n",
    "3. Implementing a user interface\n",
    "4. Connecting to databases or other data sources\n",
    "\n",
    "For a more advanced example, check out the [Multi Agent Collaboration](multi_agent_collaboration.ipynb) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
