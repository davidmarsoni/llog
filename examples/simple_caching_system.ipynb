{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcacdbe0",
   "metadata": {},
   "source": [
    "# Simple Caching System with Pickle Files\n",
    "\n",
    "This notebook demonstrates how to implement a simple caching system using pickle (.pkl) files for storing both document data and vector indices. This approach helps avoid the slowness of the notion API and reduces the cost of repeated API calls to embbed documents.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "- A Notion integration token (create one at https://www.notion.so/my-integrations)\n",
    "- An OpenAI API key\n",
    "- The Notion page must grant access to your integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ee1f9",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages. Run the cell below to install all dependencies needed for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d573f914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install llama-index llama-index-readers-notion llama-index-llms-openai python-dotenv\n",
    "\n",
    "# Verify installations\n",
    "import importlib\n",
    "\n",
    "def check_package(package_name):\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "packages = {\n",
    "    \"llama_index\": \"llama-index core\",\n",
    "    \"llama_index.readers.notion\": \"Notion reader\",\n",
    "    \"llama_index.llms.openai\": \"OpenAI integration\",\n",
    "    \"openai\": \"OpenAI API\"\n",
    "}\n",
    "\n",
    "all_installed = True\n",
    "for package, display_name in packages.items():\n",
    "    installed = check_package(package)\n",
    "    print(f\"{display_name}: {'✅ Installed' if installed else '❌ Not installed'}\")\n",
    "    all_installed = all_installed and installed\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\n✅ All required packages are installed!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Some packages are missing. Run the installation command again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd7148",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Load environment variables from the `.env` file. <br>\n",
    "N.b. it will look through the entire project for a valid `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67180ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys from environment variables\n",
    "NOTION_INTEGRATION_TOKEN = os.getenv(\"NOTION_INTEGRATION_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Cache Expiration setting  \n",
    "CACHE_DOCUMENTS_EXPIRATION = 60 * 60 * 24 * 7  # 1 week\n",
    "CACHE_INDEX_EXPIRATION = 60 * 60 * 24 * 7  # 1 week\n",
    "CACHE_QUERIES_EXPIRATION = 60 * 60 # 1 hour\n",
    "\n",
    "# Set Notion page IDs (comma-separated string if multiple one)\n",
    "page_ids_str = \"9917363395904835a604ca7a6a358579\" # replace with your Notion page ID(s)\n",
    "# Convert comma-separated string to list\n",
    "NOTION_PAGE_IDS = page_ids_str.split(\",\")\n",
    "\n",
    "# Set environment variables for compatibility with libraries that expect them\n",
    "os.environ[\"NOTION_INTEGRATION_TOKEN\"] = NOTION_INTEGRATION_TOKEN or \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY or \"\"\n",
    "\n",
    "# Verify API keys are set\n",
    "if not NOTION_INTEGRATION_TOKEN:\n",
    "    print(\"⚠️ Warning: NOTION_INTEGRATION_TOKEN is not set in .env file\")\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"⚠️ Warning: OPENAI_API_KEY is not set in .env file\")\n",
    "else:\n",
    "    print(\"✅ API keys are set\")\n",
    "    print(f\"✅ Using Notion page IDs: {NOTION_PAGE_IDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d616bf8",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "First, we'll import the necessary libraries and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba5ea9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import openai\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b45383",
   "metadata": {},
   "source": [
    "## Verify API Keys\n",
    "\n",
    "Before proceeding, let's verify that our API keys are properly set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aaa109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "if not NOTION_INTEGRATION_TOKEN:\n",
    "    raise ValueError(\"No Notion integration token found. Please set NOTION_INTEGRATION_TOKEN above.\")\n",
    "    \n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"No OpenAI API key found. Please set OPENAI_API_KEY above.\")\n",
    "\n",
    "print(\"✅ API keys verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a86007",
   "metadata": {},
   "source": [
    "## Configure LLM\n",
    "\n",
    "Set up the OpenAI language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2ca49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# Initialize the language model\n",
    "llm = OpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY, temperature=0.0, max_tokens=1000)\n",
    "\n",
    "# Set the LLM in the settings of llama_index\n",
    "Settings.llm = llm\n",
    "\n",
    "# Test the LLM\n",
    "response = llm.complete(\"Hello, I am a language model. \")\n",
    "print(\"LLM Test Response:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00309395",
   "metadata": {},
   "source": [
    "## Basic Caching Implementation\n",
    "\n",
    "Let's create a simple caching system for storing and retrieving data. We'll use pickle to serialize and deserialize Python objects to/from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import os\n",
    "\n",
    "class SimpleCache:\n",
    "    def __init__(self, cache_dir=\"cache\", expiration_seconds=CACHE_DOCUMENTS_EXPIRATION):\n",
    "        \"\"\"Initialize a simple cache with specified directory and expiration time.\"\"\"\n",
    "        self.cache_dir = cache_dir\n",
    "        self.expiration_seconds = expiration_seconds\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "    def get(self, key, data_loader=None):\n",
    "        \"\"\"Get data from cache or load it using data_loader if expired or missing.\"\"\"\n",
    "        cache_file = os.path.join(self.cache_dir, f\"{key}.pkl\")\n",
    "        \n",
    "        # Check if cache file exists and is not expired\n",
    "        if os.path.exists(cache_file):\n",
    "            file_age = time.time() - os.path.getmtime(cache_file)\n",
    "            if file_age < self.expiration_seconds:\n",
    "                print(f\"Loading {key} from cache...\")\n",
    "                with open(cache_file, \"rb\") as f:\n",
    "                    return pickle.load(f)\n",
    "            else:\n",
    "                print(f\"Cache for {key} expired ({file_age:.0f} seconds old)\")\n",
    "        else:\n",
    "            print(f\"No cache found for {key}\")\n",
    "            \n",
    "        # If we reached here, we need to load fresh data\n",
    "        if data_loader is None:\n",
    "            return None\n",
    "            \n",
    "        print(f\"Fetching fresh data for {key}...\")\n",
    "        data = data_loader()\n",
    "        self.set(key, data)\n",
    "        return data\n",
    "    \n",
    "    def set(self, key, data):\n",
    "        \"\"\"Save data to cache.\"\"\"\n",
    "        cache_file = os.path.join(self.cache_dir, f\"{key}.pkl\")\n",
    "        print(f\"Saving data to {cache_file}\")\n",
    "        with open(cache_file, \"wb\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        return True\n",
    "    \n",
    "    def invalidate(self, key):\n",
    "        \"\"\"Remove an item from the cache.\"\"\"\n",
    "        cache_file = os.path.join(self.cache_dir, f\"{key}.pkl\")\n",
    "        if os.path.exists(cache_file):\n",
    "            os.remove(cache_file)\n",
    "            print(f\"Removed {key} from cache\")\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all items from the cache.\"\"\"\n",
    "        for filename in os.listdir(self.cache_dir):\n",
    "            if filename.endswith(\".pkl\"):\n",
    "                os.remove(os.path.join(self.cache_dir, filename))\n",
    "        print(\"Cache cleared\")\n",
    "\n",
    "# Create a cache instance\n",
    "cache = SimpleCache(cache_dir=\"simple_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f16d559",
   "metadata": {},
   "source": [
    "## Test Basic Cache Functions\n",
    "\n",
    "Let's test our caching system with some basic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Define a function that simulates expensive computation\n",
    "def expensive_computation():\n",
    "    print(\"Performing expensive computation...\")\n",
    "    time.sleep(2)  # Simulate work\n",
    "    return {\n",
    "        \"result\": random.randint(1, 100),\n",
    "        \"timestamp\": time.time()\n",
    "    }\n",
    "\n",
    "# First call - should compute\n",
    "result1 = cache.get(\"test_data\", expensive_computation)\n",
    "print(f\"Result 1: {result1}\\n\")\n",
    "\n",
    "# Second call - should use cache\n",
    "result2 = cache.get(\"test_data\", expensive_computation)\n",
    "print(f\"Result 2: {result2}\\n\")\n",
    "\n",
    "# Invalidate and try again - should compute\n",
    "cache.invalidate(\"test_data\")\n",
    "result3 = cache.get(\"test_data\", expensive_computation)\n",
    "print(f\"Result 3: {result3}\\n\")\n",
    "\n",
    "# Check that results match expectations\n",
    "print(f\"Result 1 and 2 are identical: {result1 == result2}\")\n",
    "print(f\"Result 2 and 3 are different: {result2 != result3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c69104",
   "metadata": {},
   "source": [
    "## Caching Notion Data\n",
    "\n",
    "Now let's apply our caching system to Notion data retrieval. This is especially useful since Notion API calls can be slow and rate-limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbc854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.notion import NotionPageReader\n",
    "\n",
    "# Function to load data from Notion\n",
    "def load_notion_data():\n",
    "    # Get page IDs from environment or use default\n",
    "    page_ids_str = \"9917363395904835a604ca7a6a358579\"\n",
    "    page_ids = page_ids_str.split(\",\")\n",
    "    \n",
    "    print(f\"Fetching data from Notion API for pages: {page_ids}\")\n",
    "    documents = NotionPageReader(integration_token=NOTION_INTEGRATION_TOKEN).load_data(\n",
    "        page_ids=page_ids\n",
    "    )\n",
    "    print(f\"Fetched {len(documents)} documents from Notion\")\n",
    "    return documents\n",
    "\n",
    "# Try to get Notion data from cache or load it\n",
    "try:\n",
    "    # Use configurable cache expiration from .env\n",
    "    notion_cache = SimpleCache(cache_dir=\"notion_cache\", expiration_seconds=CACHE_DOCUMENTS_EXPIRATION)\n",
    "    documents = notion_cache.get(\"notion_docs\", load_notion_data)\n",
    "    \n",
    "    if documents:\n",
    "        print(f\"\\nSuccessfully loaded {len(documents)} documents\")\n",
    "        # Display brief information about the documents\n",
    "        for i, doc in enumerate(documents):\n",
    "            print(f\"Document {i+1} - Title: {doc.metadata.get('title', 'Untitled')}\")\n",
    "            print(f\"  - First 100 chars: {doc.text[:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Notion data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecba399",
   "metadata": {},
   "source": [
    "## Caching Vector Indices\n",
    "\n",
    "Vector indices can be computationally expensive to create. Let's enhance our approach to cache not just the documents but also the vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae85513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = SentenceSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Function to create vector index from documents\n",
    "def create_vector_index(documents):\n",
    "    print(\"Creating vector index...\")\n",
    "    return VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        transformations=[text_splitter]\n",
    "    )\n",
    "\n",
    "# Get documents (from cache if available)\n",
    "try:\n",
    "    documents = notion_cache.get(\"notion_docs\", load_notion_data)\n",
    "    \n",
    "    # Create a vector index cache with configurable expiration\n",
    "    index_cache = SimpleCache(cache_dir=\"vector_cache\", expiration_seconds=CACHE_INDEX_EXPIRATION)\n",
    "    \n",
    "    # Get or create index\n",
    "    index = index_cache.get(\"notion_index\", lambda: create_vector_index(documents))\n",
    "    \n",
    "    print(\"Vector index ready for querying\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up vector index: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df78b4",
   "metadata": {},
   "source": [
    "## Query the Cached Index\n",
    "\n",
    "Now let's use our cached vector index to query the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a query engine with our cached index\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=2  # Return top 2 matching chunks\n",
    ")\n",
    "\n",
    "# Define a query function with caching\n",
    "def cached_query(query_text):\n",
    "    # Use a cache for queries with 1-hour expiration\n",
    "    query_cache = SimpleCache(cache_dir=\"query_cache\", expiration_seconds=CACHE_QUERIES_EXPIRATION)\n",
    "    cache_key = f\"query_{hash(query_text)}\"  # Create a unique key for this query\n",
    "    \n",
    "    # Define a function to execute the query if not cached\n",
    "    def execute_query():\n",
    "        print(f\"Executing query: '{query_text}'\")\n",
    "        return query_engine.query(query_text)\n",
    "    \n",
    "    # Get from cache\n",
    "    result = query_cache.get(cache_key, execute_query)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"Query successful!\")\n",
    "        return result\n",
    "    else:\n",
    "        print(\"No result found for this query.\")\n",
    "        return None\n",
    "    \n",
    "# Example query\n",
    "query_text = \"What is the purpose of this document?\"\n",
    "\n",
    "# Call the cached query function\n",
    "result = cached_query(query_text)\n",
    "if result:\n",
    "    print(f\"Query result: {result}\")\n",
    "    \n",
    "    \n",
    "query_text = \"Summarize the main points of this document.\"\n",
    "\n",
    "# Call the cached query function\n",
    "result = cached_query(query_text)\n",
    "if result:\n",
    "    print(f\"Query result: {result}\")\n",
    "    \n",
    "\n",
    "# Clean up caches if needed\n",
    "# cache.clear()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28951a2a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to :\n",
    "1. Create a simple caching system using pickle files.\n",
    "2. Cache Notion data to avoid repeated API calls.\n",
    "3. Cache vector indices to speed up document retrieval and reduce computational costs.\n",
    "4. Query the cached index to retrieve relevant documents.\n",
    "\n",
    "This approach can significantly improve the performance of applications that rely on external APIs and large datasets. By caching data locally, we can reduce latency and API costs while maintaining the flexibility of our application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
