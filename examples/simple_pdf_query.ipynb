{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b7f4a4d",
   "metadata": {},
   "source": [
    "# Simple PDF Query Tool\n",
    "\n",
    "This notebook demonstrates how to create a powerful query system for PDF documents using LlamaIndex. It will show:\n",
    "\n",
    "1. How to load PDF documents\n",
    "2. How to create vector indices for these documents\n",
    "3. How to use SubQuestionQueryEngine to query across different PDF sources\n",
    "4. How to customize the system with different LLMs and prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1f3ae",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, you need:\n",
    "- An OpenAI API key\n",
    "- PDF documents to query <br> (if you put them other than in the examples folder, please change the paths to said file in the corresponding cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae66838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install llama-index llama-index-llms-openai python-dotenv openai nest-asyncio\n",
    "\n",
    "# Verify installations\n",
    "import importlib\n",
    "\n",
    "def check_package(package_name):\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "packages = {\n",
    "    \"llama_index\": \"llama-index core\",\n",
    "    \"llama_index_llms_openai\": \"llama-index-llms-openai\",\n",
    "    \"python_dotenv\": \"python-dotenv\",\n",
    "    \"openai\": \"OpenAI API\",\n",
    "    \"nest_asyncio\": \"nest-asyncio\", \n",
    "}\n",
    "\n",
    "all_installed = True\n",
    "for package, display_name in packages.items():\n",
    "    installed = check_package(package)\n",
    "    print(f\"{display_name}: {'✅ Installed' if installed else '❌ Not installed'}\")\n",
    "    all_installed = all_installed and installed\n",
    "\n",
    "if all_installed:\n",
    "    print(\"\\n✅ All required packages are installed!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Some packages are missing. Run the installation command again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366a8104",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Load environment variables from the `.env` file and set up for PDF processing. <br>\n",
    "N.b. it will look through the entire project for a valid `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab789ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops (needed for some async operations)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API keys from environment variables or set them directly\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# If environment variables are not loaded, you can set them here\n",
    "# OPENAI_API_KEY = \"your-openai-api-key\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY or \"\"\n",
    "\n",
    "# Verify API key is set\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"⚠️ Warning: OPENAI_API_KEY is not set\")\n",
    "else:\n",
    "    print(\"✅ API key is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3905caf0",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Let's import all the libraries we'll need for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616563ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core LlamaIndex components\n",
    "from llama_index.core import SimpleDirectoryReader, ServiceContext, Settings, VectorStoreIndex, SummaryIndex\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "# Import OpenAI LLM\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Import other utilities\n",
    "import logging\n",
    "import sys\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e025a",
   "metadata": {},
   "source": [
    "## Configure LLM\n",
    "\n",
    "Set up the language model we'll use for our queries and indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI LLM\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",  # You can change this to another model like \"gpt-3.5-turbo\"\n",
    "    temperature=0.2,      # Lower temperature for more consistent results\n",
    "    streaming=True,       # Enable streaming for better UX\n",
    "    system_prompt=\"You are a helpful assistant that provides accurate information about topics found in the documents.\"\n",
    ")\n",
    "\n",
    "# Set up the global LlamaIndex configuration\n",
    "Settings.llm = llm\n",
    "\n",
    "print(f\"✅ LLM configured: {llm.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e0f43",
   "metadata": {},
   "source": [
    "## Load and Index PDF Documents\n",
    "\n",
    "Let's load some PDF documents and create vector indices for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0b41d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to our PDF files\n",
    "# Update these paths to match your file locations\n",
    "pdf_paths = {\n",
    "    \"japanese_destroyers_1\": \"Imperial Japanese Navy Destroyers 1919–45 (1) Minekaze to Shiratsuyu Classes (Mark Stille, Paul Wright (Illustrator)) (Z-Library).pdf\",\n",
    "    \"japanese_destroyers_2\": \"Imperial Japanese Navy Destroyers 1919–45 (2) Asashio to Tachibana Classes (Mark Stille, Paul Wright (Illustrator)) (Z-Library).pdf\"\n",
    "    # Add more PDF files here as needed\n",
    "}\n",
    "\n",
    "# Check if the files exist\n",
    "pdf_exists = {}\n",
    "for key, path in pdf_paths.items():\n",
    "    exists = os.path.exists(path)\n",
    "    pdf_exists[key] = exists\n",
    "    if not exists:\n",
    "        print(f\"⚠️ Warning: {key} PDF file not found at {path}\")\n",
    "\n",
    "# Only proceed with files that exist\n",
    "pdf_documents = {}\n",
    "pdf_indices = {}\n",
    "\n",
    "for key, path in pdf_paths.items():\n",
    "    if pdf_exists[key]:\n",
    "        try:\n",
    "            print(f\"Loading {key} document...\")\n",
    "            pdf_documents[key] = SimpleDirectoryReader(input_files=[path]).load_data()\n",
    "            print(f\"✅ Successfully loaded {len(pdf_documents[key])} pages from {key}\")\n",
    "            \n",
    "            print(f\"Creating vector index for {key}...\")\n",
    "            pdf_indices[key] = VectorStoreIndex.from_documents(pdf_documents[key])\n",
    "            print(f\"✅ Successfully created index for {key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {key}: {e}\")\n",
    "\n",
    "print(f\"Total PDF indices created: {len(pdf_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0814074",
   "metadata": {},
   "source": [
    "## Create Query Engines\n",
    "\n",
    "Let's create individual query engines for each PDF source, then combine them into a SubQuestionQueryEngine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engines for each PDF index\n",
    "pdf_query_engines = {}\n",
    "for key, index in pdf_indices.items():\n",
    "    pdf_query_engines[key] = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "# Create a list of query engine tools\n",
    "query_engine_tools = []\n",
    "\n",
    "# Add PDF query engines to the tools list\n",
    "for key, engine in pdf_query_engines.items():\n",
    "    display_name = key.replace(\"_\", \" \").title()\n",
    "    query_engine_tools.append(\n",
    "        QueryEngineTool(\n",
    "            query_engine=engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=key, \n",
    "                description=f\"Provides information about {display_name}\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(f\"✅ Created {len(query_engine_tools)} query engine tools\")\n",
    "\n",
    "# Create the SubQuestionQueryEngine\n",
    "multi_source_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools\n",
    ")\n",
    "\n",
    "print(\"✅ Created SubQuestionQueryEngine that can query across all PDF sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2494368",
   "metadata": {},
   "source": [
    "## Query Your PDF Documents\n",
    "\n",
    "Now we can query all our PDF data sources at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_system(query_text):\n",
    "    \"\"\"Query the multi-source PDF system and display the response.\"\"\"\n",
    "    print(f\"Querying: '{query_text}'\")\n",
    "    print(\"\\nThinking...\\n\")\n",
    "    \n",
    "    # Set logging to DEBUG to show the sub-questions\n",
    "    logging.getLogger().setLevel(logging.DEBUG)\n",
    "    \n",
    "    # Execute the query\n",
    "    response = multi_source_engine.query(query_text)\n",
    "    \n",
    "    # Reset logging level\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "    # Display the response\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    display(Markdown(f\"**Answer:**\\n\\n{response}\"))\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example query about Japanese destroyers\n",
    "if \"japanese_destroyers_1\" in pdf_query_engines:\n",
    "    query_text = \"What was the Shiratsuyu class destroyer?\"\n",
    "    response = query_system(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdfd9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query that might combine information from multiple PDF sources\n",
    "if len(query_engine_tools) > 1:\n",
    "    query_text = \"Compare the Japanese naval vessels of the 1930s with their role in military strategy.\"\n",
    "    response = query_system(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0aff8d",
   "metadata": {},
   "source": [
    "## Querying Specific PDF Sources\n",
    "\n",
    "You can also query individual PDF sources directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff4591d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query just one of the PDF indices\n",
    "if \"japanese_destroyers_2\" in pdf_query_engines:\n",
    "    print(\"Querying just the second destroyer data...\")\n",
    "    destroyers_engine = pdf_query_engines[\"japanese_destroyers_2\"]\n",
    "    \n",
    "    query_text = \"What were the main Japanese destroyers of World War II?\"\n",
    "    response = destroyers_engine.query(query_text)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    display(Markdown(f\"**Destroyer Information:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd528387",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to:\n",
    "1. Load and index PDF documents using LlamaIndex\n",
    "2. Create query engines for each PDF document\n",
    "3. Combine these query engines into a multi-source research system\n",
    "4. Query the system to get answers that draw from all available PDF knowledge\n",
    "\n",
    "This approach is powerful for creating comprehensive research assistants that can leverage multiple PDF sources of knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
